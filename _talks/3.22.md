# 上节课

- 各自的网络结构



# 本节课

- 激活函数总结
- ResNet的基础结构

- 各自网络的Loss函数
- 各自网络的数据集
- 各自网络的实验效果



## ResNet

![image-20230322164519006](C:\Users\lemar\Desktop\Md\resources\image-20230322164519006.png)





## 激活函数

### DebluGAN-V2

- #### LeakyReLu防止梯度Dead（类似于梯度消失）

### Show Attend and Tell

- #### ReLu



### 为什么要激活函数

首先数据的分布绝大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，是在神经网络中引入非线性，强化网络的学习能力。所以激活函数的最大特点就是非线性。



### 激活函数中的一些问题

- 梯度消失

Sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。

例如对于一个10层的网络， 根据0.2510≈0.000000954，第10层的误差相对第一层卷积的参数的梯度将是一个非常小的值，这就是所谓的“梯度消失”。

- 梯度爆炸

与梯度消失相反，出现了梯度极大的情况

- 非零均值分布

即有的激活函数求导后的函数图像，不以零为平均值，在反向传播时可能会导致原始的分布变化

- 神经元Dead

即当梯度为0时，会导致神经元无法学习该部分特征



### 怎么选择激活函数

Sigmoid和tanh的特点是将输出限制在(0,1)和(-1,1)之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门。

而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。同样，根据ReLU的特征，Relu适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。



### 不同激活函数间讲解

- Sigmoid
- tanh
- ReLu
- Leaky ReLu



#### Sigmoid(Logistic函数)

- 将输出映射到（0，1）之间

- Sigmoid导数取值范围是[0, 0.25]

- 不是零均值分布

  <img src="C:\Users\lemar\Desktop\Md\resources\image-20230322151634480.png" alt="image-20230322151634480" style="zoom: 150%;" />

![img](https://pic2.zhimg.com/80/v2-595feb9c4660fdee432dcd30b8256735_720w.webp)

#### tanh 双曲正切函数

- 将输出映射到（-1，1）之间
- tanh导数取值范围是[0, 1]
- 计算复杂，设计幂运算和除法
- 是零均值分布
- 缓解了梯度消失问题

<img src="C:\Users\lemar\Desktop\Md\resources\image-20230322152115373.png" alt="image-20230322152115373" style="zoom:150%;" />

![img](https://pic1.zhimg.com/80/v2-ac5875cf045fbdf213b8b0ba67f10b30_720w.webp)

#### ReLu (Rectified Linear Unit)——修正线性单元函数

- 将输出映射到（0，+∞）之间
- ReLu导数取值范围是[0, 1]
- 计算简单
- 不是零均值分布
- 解决了梯度消失的问题
- 当输出为负数时，会造成屏蔽一些特征

<img src="C:\Users\lemar\Desktop\Md\resources\image-20230322152534502.png" alt="image-20230322152534502" style="zoom:200%;" />

![img](C:\Users\lemar\Desktop\Md\resources\v2-da3babf705f525effbaab3bbbed7df51_720w.webp)



#### LeakyReLu

- 将输出映射到（-∞，+∞）之间
- LeakyReLu导数取值范围是[-α, 1]
- 计算简单
- 不是零均值分布
- 解决了梯度消失的问题
- 解决了当输出为负数时，会造成屏蔽一些特征的情况



<img src="C:\Users\lemar\Desktop\Md\resources\image-20230322153030539.png" alt="image-20230322153030539" style="zoom:150%;" />



## Trick

### DebluGAN-V2

#### Gradient Penalty

解决梯度爆炸的问题

Gradient Penalty是一种用于训练GANs（生成式对抗网络）的技术，其主要作用是提高GANs的稳定性和生成质量。在GANs的训练过程中，生成器和判别器之间的梯度传播很容易变得不稳定，导致模型无法收敛或者产生低质量的生成结果。为了解决这个问题，Gradient Penalty引入了一个梯度惩罚项，用于约束判别器的梯度大小。具体来说，对于判别器的一个样本$x$，Gradient Penalty的损失函数可以定义为：

$$L_{GP}=\lambda \cdot\left(\left(||\nabla_{\tilde{x}}D(\tilde{x})||_2-1\right)^2\right)$$

其中，$\tilde{x}$表示在$x$的两个随机点之间做线性插值得到的样本，$\lambda$表示梯度惩罚参数，$D(\tilde{x})$表示判别器对于样本$\tilde{x}$的判别结果。

Gradient Penalty的作用在于约束判别器的梯度大小，使得判别器的梯度不会变得过大或过小，从而增加GANs的稳定性和生成质量。



### Show Attend and Tell

#### Clip Gradient

解决梯度爆炸和消失问题

$$\nabla_W=\text{clip}\left(\nabla_W, -\text{clipvalue}, \text{clipvalue}\right)$$

其中，$\nabla_W$表示权重矩阵$W$的梯度，clipvalue表示裁剪的范围。

Clip Gradient的作用在于限制梯度的范数，防止梯度爆炸或者消失，从而保证深度神经网络的稳定性和收敛性。

因此，虽然Gradient Penalty和Clip Gradient都是用于处理梯度问题的技术，但它们的实现方式、目的和作用不同。Gradient Penalty主要用于GANs的训练，用于约束判别器的梯度大小，增加GANs的稳定性和生成质量；而Clip Gradient主要用于深度神经网络的训练，用于限制梯度的范数，防止梯度爆炸或者消失，保证网络的稳定性和收敛性。